# AI/ML note by Zaw

# ğŸ” Common Layer Patterns

ğŸ§± Pattern 1: MLP (Multi-Layer Perceptron)
```
Input â†’ Linear â†’ ReLU â†’ Linear â†’ ReLU â†’ Linear â†’ Softmax
```
ğŸ§± Pattern 2: With Regularization
```
Input â†’ Linear â†’ BatchNorm â†’ ReLU â†’ Dropout â†’ Linear â†’ ReLU â†’ Output
```
ğŸ“· Pattern 3: CNN Block
```
Input Image
  â†“
Conv2D â†’ BatchNorm2D â†’ ReLU â†’ MaxPool
  â†“
Repeat...
  â†“
Flatten â†’ Dense â†’ Output
```
ğŸ“– Pattern 4: Transformer Block
```
Input Embeddings
  â†“
Self-Attention â†’ Add & Norm â†’ Feedforward â†’ Add & Norm
```


# ğŸš« Examples of Bad/Invalid Layer Orderings

âŒ ReLU â†’ Linear: Non-linearity before linear transformation breaks pattern.

âŒ Softmax in hidden layers: Use softmax only at the output layer for classification.

âŒ Dropout before BatchNorm: Dropout may ruin the statistics needed by BatchNorm.

âŒ BatchNorm after output layer: Output layer should produce raw logits or class probs without normalization.

# âœ… Summary Table: What Goes After What

| Layer |	Usually Followed By |
| --- | --- |
| Linear |	BatchNorm or Activation |
| BatchNorm |	Activation |
| Activation	| Dropout or Linear |
| Dropout	| Linear |
| Conv2D	| BatchNorm â†’ Activation â†’ Pool |
| Recurrent	| Dropout or another RNN block |
| Output	| Sigmoid (binary) or Softmax (multiclass) |



## ğŸ§  1. Activation Functions

These make neural networks non-linear (learn complex patterns).

Name	Use Case	Output Range
ReLU	Default for hidden layers	[0, âˆ)
Sigmoid	Binary classification	[0, 1]
Tanh	Balanced activations	[-1, 1]
Softmax	Multiclass classification	Probabilities summing to 1

ğŸ‘‰ Tip: Use ReLU for hidden layers, and only use sigmoid/softmax at the output layer if needed.

## ğŸ”¢ 2. Loss Functions

They guide learning â€” you minimize these during training.

Task Type	Loss Function
Regression	MSE (Mean Squared Error)
Binary Classification	BCE (Binary Cross Entropy)
Multiclass Classification	CrossEntropyLoss

## ğŸ§® 3. Weight Initialization

Proper weight init helps training converge faster. PyTorch/TF does this for you automatically with methods like:

Xavier (Glorot) initialization

He initialization (for ReLU)

## ğŸ§ª 4. Overfitting & Underfitting

Problem	Sign	Fixes
Overfitting	Train accuracy >> test accuracy	Add dropout, regularization, more data
Underfitting	Both accuracies low	Bigger model, longer training, tune LR

## ğŸš€ 5. Optimizer Choice

Optimizers adjust weights during training:

Optimizer	Strengths
SGD	Simple, but slow convergence
Adam	Most commonly used, fast & adaptive
RMSProp	Used in RNNs

ğŸ‘‰ Adam is a good default.

## ğŸ§ª 6. Learning Rate

Too small = slow learning
Too big = unstable training
âœ… Tip: use learning rate scheduling or try lr_find in tools like PyTorch Lightning or FastAI.

## â³ 7. Epochs, Batches, and Steps

Batch: Subset of data processed at once (affects speed and generalization)

Epoch: One full pass over the dataset

Steps: Batches per epoch

## ğŸ“Š 8. Metrics

Use the right metrics to evaluate your model:

Task	Metrics
Classification	Accuracy, Precision, Recall, F1
Regression	MSE, MAE, RÂ²

## ğŸ§± 9. Model Types to Learn About

MLP (for tabular data)

CNN (images)

RNN / LSTM / GRU (sequences)

Transformer / Attention (modern NLP and vision)

## âš™ï¸ 10. Frameworks to Know

| Library | Use Case | 
| --- | --- |
| PyTorch |	Research & flexibility |
| TensorFlow | Production & scalability |
| Keras	| Beginner-friendly (TF-based) |
| JAX	| Fast gradient computation (research/ML systems) |

## ğŸ’¡ Bonus: Tips
Use GPU (CUDA) for faster training.

Use early stopping to avoid overfitting.

Use normalization on your input data (standard scaling for MLPs, mean/std for images).

Visualize training with TensorBoard or Weights & Biases (wandb).

Save models with .pt or .pth (torch.save()), and load them with torch.load().


