# AI/ML notes by Zaw

# ğŸ” Common Layer Patterns

ğŸ§± Pattern 1: MLP (Multi-Layer Perceptron)
```
Input â†’ Linear â†’ ReLU â†’ Linear â†’ ReLU â†’ Linear â†’ Softmax
```
ğŸ§± Pattern 2: With Regularization
```
Input â†’ Linear â†’ BatchNorm â†’ ReLU â†’ Dropout â†’ Linear â†’ ReLU â†’ Output
```
ğŸ“· Pattern 3: CNN Block
```
Input Image
  â†“
Conv2D â†’ BatchNorm2D â†’ ReLU â†’ MaxPool
  â†“
Repeat...
  â†“
Flatten â†’ Dense â†’ Output
```
ğŸ“– Pattern 4: Transformer Block
```
Input Embeddings
  â†“
Self-Attention â†’ Add & Norm â†’ Feedforward â†’ Add & Norm
```


# ğŸš« Examples of Bad/Invalid Layer Orderings

âŒ ReLU â†’ Linear: Non-linearity before linear transformation breaks pattern.

âŒ Softmax in hidden layers: Use softmax only at the output layer for classification.

âŒ Dropout before BatchNorm: Dropout may ruin the statistics needed by BatchNorm.

âŒ BatchNorm after output layer: Output layer should produce raw logits or class probs without normalization.
